{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mahmoud\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mahmoud\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained VGG19 model\n",
    "vgg = models.vgg19(pretrained=True).features.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define content and style layers\n",
    "content_layers = ['conv4_2']\n",
    "style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1).to(device)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1).to(device)\n",
    "\n",
    "    def forward(self, img):\n",
    "        self.mean = self.mean.to(img.device)\n",
    "        self.std = self.std.to(img.device)\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "# Mean and standard deviation of ImageNet dataset\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create normalization module\n",
    "normalization = Normalization(mean, std).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess images\n",
    "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
    "    image = Image.open(image_path)\n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = transforms.ToTensor()(image).unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "# Content and style loss functions\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n",
    "    \n",
    "# Gram matrix calculation\n",
    "def gram_matrix(input):\n",
    "    batch_size, channels, height, width = input.size()\n",
    "    features = input.view(batch_size * channels, height * width)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(batch_size * channels * height * width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img= load_image(\"brown-and-red-concrete-building-3075532.jpg\")\n",
    "style_img=load_image(\"starry-night-1093721_1280.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with content and style layers\n",
    "content_losses = []\n",
    "style_losses = []\n",
    "\n",
    "# Move the normalization module to the GPU\n",
    "normalization.to(device)\n",
    "\n",
    "model = nn.Sequential(normalization)\n",
    "model.to(device)  # Move the model to the GPU\n",
    "\n",
    "i = 0\n",
    "for layer in vgg.children():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        i += 1\n",
    "        name = 'conv{}_{}'.format(i, i)\n",
    "    elif isinstance(layer, nn.ReLU):\n",
    "        name = 'relu{}_{}'.format(i, i)\n",
    "        layer = nn.ReLU(inplace=False)\n",
    "    elif isinstance(layer, nn.MaxPool2d):\n",
    "        name = 'pool_{}'.format(i)\n",
    "    elif isinstance(layer, nn.BatchNorm2d):\n",
    "        name = 'bn_{}'.format(i)\n",
    "    else:\n",
    "        raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "    layer = layer.to(device)  # Move the layer to the GPU\n",
    "\n",
    "    model.add_module(name, layer)\n",
    "\n",
    "    if name in content_layers:\n",
    "        # Move content_img to GPU\n",
    "        content_img = content_img.to(device)\n",
    "        target = model(content_img).detach()\n",
    "        content_loss = ContentLoss(target)\n",
    "        model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "        content_losses.append(content_loss)\n",
    "\n",
    "    if name in style_layers:\n",
    "        # Move style_img to GPU\n",
    "        style_img = style_img.to(device)\n",
    "        target_feature = model(style_img).detach()\n",
    "        style_loss = StyleLoss(target_feature)\n",
    "        model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "        style_losses.append(style_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "target =load_image(\"OIP.jpeg\")\n",
    "# Remove the layers after the last content and style losses\n",
    "for i in range(len(model) - 1, -1, -1):\n",
    "    if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "        break\n",
    "\n",
    "model = model[:(i + 1)]\n",
    "\n",
    "# Optimization parameters\n",
    "optimizer = torch.optim.Adam([target], lr=0.03, betas=[0.5, 0.999])\n",
    "num_steps = 500\n",
    "style_weight = 1000000\n",
    "content_weight = 1\n",
    "\n",
    "# Style transfer\n",
    "for step in range(num_steps):\n",
    "    target_features = model(target)\n",
    "    content_loss = 0\n",
    "    style_loss = 0\n",
    "\n",
    "    for cl in content_losses:\n",
    "        content_loss += cl.loss\n",
    "    for sl in style_losses:\n",
    "        style_loss += sl.loss\n",
    "\n",
    "    content_loss *= content_weight\n",
    "    style_loss *= style_weight\n",
    "\n",
    "    total_loss = content_loss + style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if step % 50 == 0:\n",
    "    #     print(\"Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}\".format(step + 1, num_steps, content_loss, style_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the stylized image\n",
    "utils.save_image(target, 'stylized_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(target.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
